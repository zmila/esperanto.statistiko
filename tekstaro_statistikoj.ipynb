{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistikoj de Esperantaj tekstoj, prenitaj el https://tekstaro.com/elshuti.html\n",
    "La tekstoj estas kreitaj 4-an de Marto 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility methods\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def write_items_into_file(items, file_name, header=['key', 'value']):\n",
    "    \"\"\"\n",
    "    Writes a list of items into a CSV file with two columns: key and value.\n",
    "    \n",
    "    Args:\n",
    "        items (dict): The list of items to write into the file.\n",
    "        file_name (str): The name of the output CSV file.\n",
    "        header (list): The header for the CSV file.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(header)\n",
    "        csvwriter.writerows(items)\n",
    "    \n",
    "    \n",
    "def read_csv_into_dict(file_name, skip_header=True):\n",
    "    \"\"\"\n",
    "    Reads a CSV file into a dictionary with keys as the first column and values as the second column.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        defaultdict: A dictionary with keys and values from the CSV file.\n",
    "    \"\"\"\n",
    "    data_dict = defaultdict(int)\n",
    "    with open(file_name, 'r', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        if skip_header:\n",
    "            next(csvreader)\n",
    "        for row in csvreader:\n",
    "            key, value = row\n",
    "            data_dict[key] += int(value)\n",
    "    return data_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dir= ./html_kun_streketoj/tekstoj\n",
      "there are = 120 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"./html_kun_streketoj/tekstoj\"\n",
    "print(f'base_dir= {base_dir}')\n",
    "\n",
    "file_names = os.listdir(base_dir)\n",
    "print(f'there are = {len(file_names)} files')\n",
    "# for file_name in sorted(file_names):\n",
    "#     print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  read all files and extract the text between <p id[^>]+> and </p>\n",
    "#  and replace <span class=\"streketo\">•</span> with '_'\n",
    "#  and write all texts into one file 'all_texts_joined.html'\n",
    "\n",
    "import re\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    print(f'processing file: {file_name}')\n",
    "    file_name = os.path.join(base_dir, file_name)\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        fragments = [re.sub(r'^<p id[^>]+>|</p>$', '', fragment) for fragment in re.findall(r'<p id[^>]+>.*?</p>', content, re.DOTALL)]\n",
    "        fragments = [fragment.replace('<span class=\"streketo\">•</span>', '_') for fragment in fragments]\n",
    "        all_texts.extend(fragments)\n",
    "    print('   done')\n",
    "\n",
    "output_file = 'all_texts_joined.html'\n",
    "print(f'writing texts {len(all_texts)} lines into {output_file}')\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(all_texts))\n",
    "\n",
    "print(\" all done!\")\n",
    "\n",
    "# 18.4 s\n",
    "# writing texts 250518 lines into all_texts_joined.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content without html tags written to all_texts_joined.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the file 'all_texts_joined.html', remove all html tags, write the result into 'all_texts_joined.txt'\n",
    "\n",
    "with open('all_texts_joined.html', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    re_open = re.compile('<[^<]+?>')\n",
    "    without_open_tag = re.sub(re_open, '', text)\n",
    "    re_close = re.compile('</[^<]+?>')\n",
    "    without_close_tag = re.sub(re_close, '', without_open_tag)\n",
    "    return without_close_tag\n",
    "\n",
    "# Remove any HTML tag in the format \"<tag\" + any content but not \">\" till \">\" and then \"</\" till \">\"\n",
    "cleaned_content = remove_html_tags(content)\n",
    "\n",
    "# Write the result into 'all_texts_joined.txt'\n",
    "output_txt_file = 'all_texts_joined.txt'\n",
    "with open(output_txt_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_content)\n",
    "\n",
    "print(f\"Content without html tags written to {output_txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all texts without punctuation saved to 'all_texts_without_punctuation.txt'\n"
     ]
    }
   ],
   "source": [
    "# Read the file 'all_texts_joined.txt', remove all punctuation characters, digits, and extra spaces\n",
    "\n",
    "\n",
    "with open('all_texts_joined.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace all minus signs with underscores if the minus is inside a word\n",
    "content = re.sub(r'(?<=\\w)-(?=\\w)', '_', content)\n",
    "\n",
    "# Remove all punctuation characters\n",
    "content = re.sub(r'[^\\w\\s]', '', content)\n",
    "# Remove all digits\n",
    "content = re.sub(r'\\d+', '', content)\n",
    "# Remove all extra spaces and tabs\n",
    "content = re.sub(r'\\t+', ' ', content)\n",
    "content = re.sub(r'\\s+', ' ', content)\n",
    "# # Remove all new line characters\n",
    "# content = re.sub(r'\\n+', ' ', content)\n",
    "\n",
    "output_file = 'all_texts_without_punctuation.txt'\n",
    "# Write the modified content to output_file\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"all texts without punctuation saved to '{output_file}'\")\n",
    "\n",
    "# 13.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words = 12605345, Esperanto words= 12532375, unique words = 371937\n",
      "Word frequencies written to 'words_frequency.csv'\n"
     ]
    }
   ],
   "source": [
    "# calculate the frequency of each word in 'all_texts_without_punctuation.txt'\n",
    "# and write the result into 'words_frequency.csv'\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Read the file 'all_texts_without_punctuation.txt'\n",
    "with open('all_texts_without_punctuation.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content into words and convert to lowercase\n",
    "all_words = content.lower().split()\n",
    "\n",
    "# Remove all words starting with \"http\"\n",
    "all_words = [word for word in all_words if not word.startswith(\"http\")]\n",
    "\n",
    "# Calculate the frequency of each word\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Remove all words containing characters not in the Esperanto alphabet\n",
    "esperanto_chars = \"abcĉdefgĝhĥijĵklmnoprsŝtuŭvz_\"\n",
    "eo_words = [word for word in all_words if all(char in esperanto_chars for char in word)]\n",
    "\n",
    "word_counts = Counter(eo_words)\n",
    "\n",
    "print(f\"total words = {len(all_words)}, Esperanto words= {len(eo_words)}, unique words = {len(word_counts)}\")\n",
    "\n",
    "# Sort the words by frequency in descending order\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Write the result into 'words_frequency.csv'\n",
    "output_csv_file = 'words_frequency.csv'\n",
    "write_items_into_file(sorted_word_counts, output_csv_file, header=['word', 'count'])\n",
    "\n",
    "print(f\"Word frequencies written to '{output_csv_file}'\")\n",
    "\n",
    "# 16.9s\n",
    "# total words = 12605345, Esperanto words= 12532375, unique words = 371937\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized words count = 261004\n",
      "Normalized word frequencies written to 'words_frequency_normalized.csv'\n"
     ]
    }
   ],
   "source": [
    "# read csv file 'words_frequency.csv' \n",
    "# convert each word to normal form \n",
    "# put normal form into new dictionary, updating frequency, if this form is already in the dictionary\n",
    "# write the result into 'words_frequency_normalized.csv'\n",
    "\n",
    "def normalize_word(word):\n",
    "    if word.endswith(\"_n\"):\n",
    "        word = word[:-2]\n",
    "    if word.endswith(\"_j\"):\n",
    "        word = word[:-2]\n",
    "    if word.endswith((\"_as\", \"_is\", \"_os\", \"_us\")):\n",
    "        word = word[:-2] + \"i\"\n",
    "    if word.endswith((\"_u\")):\n",
    "        word = word[:-1] + \"i\"\n",
    "    return word\n",
    "\n",
    "# Read the CSV file and normalize words\n",
    "words_counts = read_csv_into_dict('words_frequency.csv')\n",
    "\n",
    "normalized_word_counts = defaultdict(int)\n",
    "for word, count in words_counts.items():\n",
    "    normalized_word = normalize_word(word)\n",
    "    normalized_word_counts[normalized_word] += int(count)\n",
    "print(f\"normalized words count = {len(normalized_word_counts)}\")\n",
    "\n",
    "# Write the normalized word frequencies to a new CSV file\n",
    "sorted_items = sorted(normalized_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "output_file_name = 'words_frequency_normalized.csv'\n",
    "write_items_into_file(sorted_items, output_file_name, header=['word', 'count'])\n",
    "\n",
    "print(f\"Normalized word frequencies written to '{output_file_name}'\")\n",
    "\n",
    "# normalized words count = 261004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of words with underscores = 199703\n",
      "Words with underscores written to 'words_with_underscore_frequency.csv'\n",
      "count of words without underscores = 61301\n",
      "Words without underscores written to 'words_without_underscore_frequency.csv'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Read the file 'words_frequency_normalized.csv' and divide the words into two dictionaries\n",
    "\n",
    "words_counts = read_csv_into_dict('words_frequency_normalized.csv')\n",
    "\n",
    "words_with_underscore = defaultdict(int)\n",
    "words_without_underscore = defaultdict(int)\n",
    "for word, count in words_counts.items():\n",
    "    count = int(count)\n",
    "    if '_' in word:\n",
    "        words_with_underscore[word] += count\n",
    "    else:\n",
    "        words_without_underscore[word] += count\n",
    "\n",
    "# Write the words with underscores to 'words_with_underscore_frequency.csv'\n",
    "print(f'count of words with underscores = {len(words_with_underscore)}')\n",
    "write_items_into_file(sorted(words_with_underscore.items(), key=lambda x: x[1], reverse=True), 'words_with_underscore_frequency.csv', header=['word', 'count'])\n",
    "print(\"Words with underscores written to 'words_with_underscore_frequency.csv'\")\n",
    "\n",
    "# Write the words without underscores to 'words_without_underscore_frequency.csv'\n",
    "print(f'count of words without underscores = {len(words_without_underscore)}')\n",
    "write_items_into_file(sorted(words_without_underscore.items(), key=lambda x: x[1], reverse=True), 'words_without_underscore_frequency.csv', header=['word', 'count'])\n",
    "print(\"Words without underscores written to 'words_without_underscore_frequency.csv'\")\n",
    "\n",
    "# count of words with underscores = 199703\n",
    "# count of words without underscores = 61301\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is manual step: look into words_without_underscore_frequency.csv, select all Esperanto words without endings (numbers, prepositions, correlative words). they are at top. about 160 upper lines.\n",
    "copy them into words_with_underscore_frequency.csv file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word,count\n",
    "la,1131201\n",
    "de,538681\n",
    "kaj,405750\n",
    "en,270668\n",
    "al,175182\n",
    "mi,149265\n",
    "ne,141823\n",
    "kiu,130081\n",
    "li,121481\n",
    "por,112062\n",
    "ke,111441\n",
    "pri,92462\n",
    "tiu,90918\n",
    "sed,74336\n",
    "vi,72739\n",
    "ili,72719\n",
    "ĝi,67035\n",
    "kun,66680\n",
    "kiel,59181\n",
    "ni,56211\n",
    "el,55851\n",
    "tio,55670\n",
    "pli,49601\n",
    "oni,46296\n",
    "ŝi,43419\n",
    "per,40944\n",
    "ĉiu,40224\n",
    "aŭ,37631\n",
    "sur,35458\n",
    "nur,35275\n",
    "ankaŭ,34969\n",
    "da,33852\n",
    "ĉu,29811\n",
    "se,29521\n",
    "dum,29069\n",
    "kiam,28706\n",
    "ĉi,28663\n",
    "ĉar,28193\n",
    "unu,27514\n",
    "plej,27333\n",
    "si,25989\n",
    "kio,25881\n",
    "pro,25086\n",
    "post,23574\n",
    "ol,22731\n",
    "ĉe,22487\n",
    "jam,22416\n",
    "nun,22008\n",
    "tiel,21865\n",
    "inter,21804\n",
    "eĉ,21760\n",
    "tre,20887\n",
    "laŭ,19923\n",
    "tamen,18970\n",
    "du,18094\n",
    "antaŭ,17922\n",
    "ĝis,16851\n",
    "tie,16528\n",
    "do,16474\n",
    "kontraŭ,14611\n",
    "tia,14264\n",
    "mem,14027\n",
    "je,13817\n",
    "kie,13198\n",
    "ankoraŭ,13057\n",
    "iu,13046\n",
    "tiam,12680\n",
    "ja,12251\n",
    "iom,11156\n",
    "ĉio,10946\n",
    "sen,10672\n",
    "ĉiam,10046\n",
    "uea,9902\n",
    "jen,9857\n",
    "neniu,8453\n",
    "plu,8183\n",
    "sub,8142\n",
    "tri,7902\n",
    "io,7832\n",
    "tuj,7658\n",
    "kies,7502\n",
    "neniam,7218\n",
    "kvankam,7129\n",
    "nek,7126\n",
    "preskaŭ,7115\n",
    "kvazaŭ,6916\n",
    "tra,6582\n",
    "tro,6577\n",
    "ia,6527\n",
    "jes,6266\n",
    "nenio,6202\n",
    "kial,6186\n",
    "tial,6071\n",
    "tiom,6042\n",
    "kia,5496\n",
    "ajn,5340\n",
    "ĉirkaŭ,4549\n",
    "nu,4516\n",
    "ho,4506\n",
    "ambaŭ,4321\n",
    "hodiaŭ,4128\n",
    "krom,4039\n",
    "dek,3985\n",
    "almenaŭ,3942\n",
    "apud,3890\n",
    "iam,3877\n",
    "kiom,3851\n",
    "super,3774\n",
    "zamenhof,3764\n",
    "ktp,3742\n",
    "kvar,3673\n",
    "malgraŭ,3479\n",
    "ties,3457\n",
    "kvin,3371\n",
    "baldaŭ,3326\n",
    "mil,3226\n",
    "anstataŭ,2932\n",
    "apenaŭ,2757\n",
    "ĉia,2753\n",
    "ĵus,2382\n",
    "nenia,2286\n",
    "des,2241\n",
    "tejo,2227\n",
    "ekster,2152\n",
    "ses,2147\n",
    "for,2115\n",
    "sep,1831\n",
    "izrael,1664\n",
    "cent,1629\n",
    "po,1511\n",
    "david,1480\n",
    "ĉie,1452\n",
    "iel,1372\n",
    "ok,1259\n",
    "neniel,1236\n",
    "trans,1164\n",
    "eksa,1147\n",
    "ci,966\n",
    "morgaŭ,933\n",
    "ju,788\n",
    "hieraŭ,746\n",
    "naŭ,726\n",
    "ve,716\n",
    "preter,471\n",
    "nenie,357\n",
    "neniom,311\n",
    "adiaŭ,311\n",
    "ĉies,305\n",
    "ajna,255\n",
    "ies,251\n",
    "far,194\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of words = 199571\n",
      "Words with frequencies written to 'words_frequency_1.csv'\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read the file 'words_with_underscore_frequency.csv' into dictionary word-> count\n",
    "words_with_frequencies = read_csv_into_dict('words_with_underscore_frequency.csv')\n",
    "        \n",
    "# Remove all starting underscores and update the dictionary\n",
    "updated_words_with_frequencies = defaultdict(int)\n",
    "for word, count in words_with_frequencies.items():\n",
    "    word = re.sub(r'__', '_', word)\n",
    "    updated_word = word.lstrip('_').rstrip('_')\n",
    "    updated_words_with_frequencies[updated_word] += int(count)\n",
    "\n",
    "# Sort the words by frequency in descending order, then by word in alphabetical order with locale Esperanto\n",
    "locale.setlocale(locale.LC_ALL, 'eo.UTF-8')\n",
    "words_with_frequencies = sorted(updated_words_with_frequencies.items(), key=lambda x: (-x[1], locale.strxfrm(x[0])))\n",
    "\n",
    "# Write the sorted words into 'words_frequency_1.csv'\n",
    "output_file_name= 'words_frequency_1.csv'\n",
    "write_items_into_file(words_with_frequencies, output_file_name, header=['word', 'count'])\n",
    "\n",
    "print(f'count of words = {len(words_with_frequencies)}')\n",
    "print(\"Words with frequencies written to 'words_frequency_1.csv'\")\n",
    "\n",
    "# 199571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove all words from `words_frequency_1.csv` which have frequency 1\n",
    "\n",
    "count of words = 93190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of word parts = 16186\n",
      "Word parts with frequencies written to 'word_parts_frequency.csv'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import locale\n",
    "\n",
    "# Read the file 'words_frequency_1.csv' into dictionary word->count\n",
    "words_with_frequencies = read_csv_into_dict('words_frequency_3.csv')\n",
    "\n",
    "part_frequencies = defaultdict(int)  # Initialize defaultdict with int as default factory\n",
    "for word, frequency in words_with_frequencies.items():\n",
    "    parts = word.split('_')  \n",
    "    num_parts = len(parts)\n",
    "    if num_parts == 1:\n",
    "        part = parts[0]\n",
    "        if part:\n",
    "            part_frequencies[part] += frequency\n",
    "    elif num_parts > 1:\n",
    "        for index, part in enumerate(parts):\n",
    "            if part: \n",
    "                if index == 0: \n",
    "                    formatted_part = part + \"-\"\n",
    "                elif index == num_parts - 1:\n",
    "                    formatted_part = \"-\" + part\n",
    "                else: \n",
    "                    formatted_part = \"-\" + part + \"-\"\n",
    "                part_frequencies[formatted_part] += frequency    \n",
    "\n",
    "# Sort the words by frequency in descending order, then by word in alphabetical order with locale Esperanto\n",
    "locale.setlocale(locale.LC_ALL, 'eo.UTF-8')\n",
    "sorted_part_frequencies = sorted(part_frequencies.items(), key=lambda x: (-x[1], locale.strxfrm(x[0])))\n",
    "\n",
    "output_file = 'word_parts_frequency.csv'\n",
    "# Write the sorted words into 'word_parts_frequency.csv'\n",
    "write_items_into_file(sorted_part_frequencies, output_file)\n",
    "\n",
    "print(f'count of word parts = {len(sorted_part_frequencies)}')\n",
    "print(f\"Word parts with frequencies written to '{output_file}'\")\n",
    "\n",
    "# count of word parts = 16186"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of prefixes = 11848\n",
      "prefixes written into 'word_parts_prefixes.csv'\n",
      "count of inner parts = 4240\n",
      "inner parts written into 'word_parts_inner_parts.csv'\n",
      "count of suffixes = 98\n",
      "suffixes written into 'word_parts_suffixes.csv'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "part_frequencies = read_csv_into_dict('word_parts_frequency.csv')\n",
    "\n",
    "#  group all parts into three dictionaries: prefixes, inner parts, suffixes\n",
    "prefixes = defaultdict(int)\n",
    "suffixes = defaultdict(int) \n",
    "inner_parts = defaultdict(int)\n",
    "for part, frequency in part_frequencies.items():\n",
    "    if part.startswith('-') and part.endswith('-'):\n",
    "        inner_parts[part] += frequency\n",
    "    elif part.startswith('-'):\n",
    "        suffixes[part] += frequency\n",
    "    elif part.endswith('-'):\n",
    "        prefixes[part] += frequency\n",
    "    else:\n",
    "        inner_parts[part] += frequency\n",
    "        \n",
    "# write each group into separate files\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'eo.UTF-8')\n",
    "sorted_prefixes = sorted(prefixes.items(), key=lambda x: (-x[1], locale.strxfrm(x[0])))\n",
    "print(f'count of prefixes = {len(sorted_prefixes)}')\n",
    "write_items_into_file(sorted_prefixes, 'word_parts_prefixes.csv', header=['prefix', 'count'])\n",
    "print(\"prefixes written into 'word_parts_prefixes.csv'\")\n",
    "\n",
    "sorted_inner_parts = sorted(inner_parts.items(), key=lambda x: (-x[1], locale.strxfrm(x[0])))\n",
    "print(f'count of inner parts = {len(sorted_inner_parts)}')\n",
    "write_items_into_file(sorted_inner_parts, 'word_parts_inner_parts.csv', header=['inner_part', 'count'])\n",
    "print(\"inner parts written into 'word_parts_inner_parts.csv'\")\n",
    "\n",
    "sorted_suffixes = sorted(suffixes.items(), key=lambda x: (-x[1], locale.strxfrm(x[0])))\n",
    "print(f'count of suffixes = {len(sorted_suffixes)}')\n",
    "write_items_into_file(sorted_suffixes, 'word_parts_suffixes.csv', header=['suffix', 'count'])\n",
    "print(\"suffixes written into 'word_parts_suffixes.csv'\")\n",
    "\n",
    "\n",
    "# count of prefixes = 11909\n",
    "# count of inner parts = 4255\n",
    "# count of suffixes = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are a lot of 'suffixes', most of them are not esperanto words.\n",
    "need manual filtering of this words and updating of file `words_frequency_1.csv`\n",
    "the result will be stored into `words_frequency_2.csv`\n",
    "\n",
    "run the following filtering code from word_frequency_2.csv or word_frequency_3.csv\n",
    "and again group by prefix,inner,suffix - until there are 100 suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total count of words = 92047\n",
      "count of words with rare suffixes = 1\n",
      "rare suffixes written into 'rare_suffixes.csv'\n",
      "removed word with rare suffix: nu_nu,2 because of suffix: nu,2\n",
      "count of words after removing words with rare suffixes = 92046\n",
      "Words with frequencies written to 'words_frequency_3.csv'\n"
     ]
    }
   ],
   "source": [
    "# Read the file 'words_frequency_2.csv' into dictionary word->count\n",
    "words_with_frequencies = read_csv_into_dict('words_frequency_3.csv')\n",
    "print(f'total count of words = {len(words_with_frequencies)}')\n",
    "\n",
    "suffixes = read_csv_into_dict('word_parts_suffixes.csv')\n",
    "\n",
    "rare_suffixes = {suffix[1:]: count for suffix, count in suffixes.items() if count < 9}\n",
    "print(f'count of words with rare suffixes = {len(rare_suffixes)}')\n",
    "\n",
    "write_items_into_file(sorted(rare_suffixes.items(), key=lambda x: x[1], reverse=True), 'rare_suffixes.csv', header=['suffix', 'count'])\n",
    "print(\"rare suffixes written into 'rare_suffixes.csv'\")\n",
    "\n",
    "# Remove words ending with rare suffixes\n",
    "words_to_delete = []\n",
    "for word,count in words_with_frequencies.items():\n",
    "    suffix = word.split('_')[-1]\n",
    "    if suffix != word and suffix in rare_suffixes:\n",
    "        words_to_delete.append(word)\n",
    "        print(f'removed word with rare suffix: {word},{count} because of suffix: {suffix},{rare_suffixes[suffix]}')\n",
    "\n",
    "# Delete the collected words after iteration\n",
    "for word in words_to_delete:\n",
    "    del words_with_frequencies[word]\n",
    "    \n",
    "print(f'count of words after removing words with rare suffixes = {len(words_with_frequencies)}')\n",
    "# Write the remaining words into 'words_frequency_3.csv'\n",
    "output_file_name = 'words_frequency_3.csv'\n",
    "write_items_into_file(sorted(words_with_frequencies.items(), key=lambda x: x[1], reverse=True), output_file_name, header=['word', 'count'])\n",
    "print(f\"Words with frequencies written to '{output_file_name}'\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
