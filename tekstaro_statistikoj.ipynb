{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistikoj de Esperantaj tekstoj, prenitaj el https://tekstaro.com/elshuti.html\n",
    "La tekstoj estas kreitaj 4-an de Marto 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dir= ./html_kun_streketoj/tekstoj\n",
      " there are = 120 files\n",
      "al-torento.html\n",
      "aldono-al-la-dogmoj-de-hilelismo.html\n",
      "aldono-dua-libro.html\n",
      "artikoloj-el-monato-2012-2018.html\n",
      "artikoloj-el-monato.html\n",
      "azia-strategio.html\n",
      "chu-li-bremsis-sufiche.html\n",
      "chu-li-venis-trakosme.html\n",
      "chu-li.html\n",
      "chu-ni-kunvenis-vane.html\n",
      "chu-rakonti-novele.html\n",
      "chu-shi-mortu-trafike.html\n",
      "chu-vi-kuiras-chine.html\n",
      "denaska-kongresano.html\n",
      "dogmoj-de-hilelismo.html\n",
      "don-kihhoto-de-la-mancho-en-barcelono.html\n",
      "dua-libro.html\n",
      "el-la-mondo-homarana.html\n",
      "el-vivo-de-chukchoj.html\n",
      "elektitaj-fabeloj.html\n",
      "esenco-kaj-estonteco.html\n",
      "esperanto-en-perspektivo.html\n",
      "fabeloj-de-andersen-1.html\n",
      "fabeloj-de-andersen-2.html\n",
      "fabeloj-de-andersen-3.html\n",
      "fabeloj-de-andersen-4.html\n",
      "fajron-sentas-mi-interne.html\n",
      "federala-sperto.html\n",
      "fundamenta-antauparolo.html\n",
      "fundamenta-ekzercaro.html\n",
      "fundamenta-krestomatio-nez.html\n",
      "fundamenta-krestomatio.html\n",
      "gentoj-kaj-lingvo-internacia.html\n",
      "georgo-dandin.html\n",
      "gerda-malaperis.html\n",
      "ghis-revido-krokodilido.html\n",
      "hamleto.html\n",
      "hitler-mau-strindberg-kaj-mi.html\n",
      "homaranismo-1906.html\n",
      "homaranismo-1913.html\n",
      "homoj-sur-la-tero.html\n",
      "idoj-de-orfeo.html\n",
      "ifigenio-en-taurido.html\n",
      "infanoj-en-torento.html\n",
      "internacia-krestomatio.html\n",
      "kastelo-de-prelongo.html\n",
      "kien-fluas-roj-castalie.html\n",
      "kion-zamenhof-ne-povis-diri-en-ghenevo.html\n",
      "koko-krias-jam.html\n",
      "kompatinda-klem.html\n",
      "kontakto-2011-2019.html\n",
      "kredu-min-sinjorino.html\n",
      "kruko-kaj-baniko.html\n",
      "la-batalo-de-l-vivo.html\n",
      "la-bona-lingvo.html\n",
      "la-esperantisto.html\n",
      "la-faraono.html\n",
      "la-gimnazio.html\n",
      "la-granda-aventuro.html\n",
      "la-kiso.html\n",
      "la-majstro-kaj-margarita.html\n",
      "la-majstro-kaj-martinelli.html\n",
      "la-memorajhoj-de-julia-agripina.html\n",
      "la-rabeno-de-bahharahh.html\n",
      "la-rabistoj.html\n",
      "la-respubliko.html\n",
      "la-revizoro.html\n",
      "la-shtona-urbo.html\n",
      "la-skandalo-pro-jozefo.html\n",
      "la-soleno.html\n",
      "leteroj-de-zamenhof-aliuloj.html\n",
      "leteroj-de-zamenhof-waringhien.html\n",
      "leteroj-de-zamenhof.html\n",
      "lingvaj-respondoj.html\n",
      "lingvistikaj-aspektoj.html\n",
      "malnova-testamento.html\n",
      "marta.html\n",
      "metropoliteno.html\n",
      "mia-penso.html\n",
      "mirinda-amo.html\n",
      "mondediplo-2002.html\n",
      "mondediplo-2005.html\n",
      "mondediplo-2008.html\n",
      "mondediplo-2011.html\n",
      "mondediplo-2014.html\n",
      "mondediplo-2017.html\n",
      "mortula-shipo.html\n",
      "ne-nur-leteroj-de-plumamikoj.html\n",
      "nefermita-letero-al-beaufront.html\n",
      "nova-testamento.html\n",
      "ombro-sur-interna-pejzagho.html\n",
      "ondo-de-esperanto.html\n",
      "originala-verkaro.html\n",
      "paroladoj-de-zamenhof.html\n",
      "patroj-kaj-filoj.html\n",
      "poezio-zamenhofa-fundamenta-krestomatio.html\n",
      "por-pli-efika-informado.html\n",
      "post-la-granda-milito.html\n",
      "pri-la-homaranismo.html\n",
      "pro-ishtar.html\n",
      "proverbaro-esperanta.html\n",
      "quo-vadis.html\n",
      "reformoj-esperanto.html\n",
      "retoriko.html\n",
      "revuo-esperanto-2002-2007.html\n",
      "revuo-esperanto-2008-2012.html\n",
      "revuo-esperanto-2013-2017.html\n",
      "robinsono-kruso.html\n",
      "satiraj-rakontoj.html\n",
      "sinjoro-tadeo.html\n",
      "tokio-invitas-vin.html\n",
      "tri-angloj-alilande.html\n",
      "unua-libro.html\n",
      "vespera-rugho-anoncas-ventegon.html\n",
      "vivo-de-zamenhof.html\n",
      "vivo-vokas.html\n",
      "vojaghimpresoj.html\n",
      "vojagho-al-kazohinio.html\n",
      "vortoj-de-lanti.html\n",
      "zamenhof.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"./html_kun_streketoj/tekstoj\"\n",
    "print(f'base_dir= {base_dir}')\n",
    "\n",
    "file_names = os.listdir(base_dir)\n",
    "print(f'there are = {len(file_names)} files')\n",
    "for file_name in sorted(file_names):\n",
    "    print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  read all files and extract the text between <p id[^>]+> and </p>\n",
    "#  and replace <span class=\"streketo\">•</span> with '_'\n",
    "#  and write all texts into one file 'all_texts_joined.html'\n",
    "\n",
    "import re\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    print(f'processing file: {file_name}')\n",
    "    file_path = os.path.join(base_dir, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        fragments = [re.sub(r'^<p id[^>]+>|</p>$', '', fragment) for fragment in re.findall(r'<p id[^>]+>.*?</p>', content, re.DOTALL)]\n",
    "        fragments = [fragment.replace('<span class=\"streketo\">•</span>', '_') for fragment in fragments]\n",
    "        all_texts.extend(fragments)\n",
    "    print('   done')\n",
    "\n",
    "output_file = 'all_texts_joined.html'\n",
    "print(f'writing texts {len(all_texts)} lines into {output_file}')\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write('\\n'.join(all_texts))\n",
    "\n",
    "print(\" all done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content without html tags written to all_texts_joined.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the file 'all_texts_joined.html', remove all html tags, write the result into 'all_texts_joined.txt'\n",
    "\n",
    "with open('all_texts_joined.html', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    re_open = re.compile('<[^<]+?>')\n",
    "    without_open_tag = re.sub(re_open, '', text)\n",
    "    re_close = re.compile('</[^<]+?>')\n",
    "    without_close_tag = re.sub(re_close, '', without_open_tag)\n",
    "    return without_close_tag\n",
    "\n",
    "# Remove any HTML tag in the format \"<tag\" + any content but not \">\" till \">\" and then \"</\" till \">\"\n",
    "cleaned_content = remove_html_tags(content)\n",
    "\n",
    "# Write the result into 'all_texts_joined.txt'\n",
    "output_txt_file = 'all_texts_joined.txt'\n",
    "with open(output_txt_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_content)\n",
    "\n",
    "print(f\"Content without html tags written to {output_txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all texts without punctuation saved to 'all_texts_without_punctuation.txt'\n"
     ]
    }
   ],
   "source": [
    "# Read the file 'all_texts_joined.txt', remove all punctuation characters, digits, and extra spaces\n",
    "\n",
    "\n",
    "with open('all_texts_joined.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Replace all minus signs with underscores if the minus is inside a word\n",
    "content = re.sub(r'(?<=\\w)-(?=\\w)', '_', content)\n",
    "\n",
    "# Remove all punctuation characters\n",
    "content = re.sub(r'[^\\w\\s]', '', content)\n",
    "# Remove all digits\n",
    "content = re.sub(r'\\d+', '', content)\n",
    "# Remove all extra spaces and tabs\n",
    "content = re.sub(r'\\t+', ' ', content)\n",
    "content = re.sub(r'\\s+', ' ', content)\n",
    "# # Remove all new line characters\n",
    "# content = re.sub(r'\\n+', ' ', content)\n",
    "\n",
    "output_file = 'all_texts_without_punctuation.txt'\n",
    "# Write the modified content to output_file\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)\n",
    "\n",
    "print(f\"all texts without punctuation saved to '{output_file}'\")\n",
    "\n",
    "# 13.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words = 12605345, Esperanto words= 12532375, unique words = 371937\n",
      "Word frequencies written to 'words_frequency.csv'\n"
     ]
    }
   ],
   "source": [
    "# calculate the frequency of each word in 'all_texts_without_punctuation.txt'\n",
    "# and write the result into 'words_frequency.csv'\n",
    "\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Read the file 'all_texts_without_punctuation.txt'\n",
    "with open('all_texts_without_punctuation.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split the content into words and convert to lowercase\n",
    "all_words = content.lower().split()\n",
    "\n",
    "# Remove all words starting with \"http\"\n",
    "all_words = [word for word in all_words if not word.startswith(\"http\")]\n",
    "\n",
    "# Calculate the frequency of each word\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Remove all words containing characters not in the Esperanto alphabet\n",
    "esperanto_chars = \"abcĉdefgĝhĥijĵklmnoprsŝtuŭvz_\"\n",
    "eo_words = [word for word in all_words if all(char in esperanto_chars for char in word)]\n",
    "\n",
    "word_counts = Counter(eo_words)\n",
    "\n",
    "print(f\"total words = {len(all_words)}, Esperanto words= {len(eo_words)}, unique words = {len(word_counts)}\")\n",
    "\n",
    "# Sort the words by frequency in descending order\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Write the result into 'words_frequency.csv'\n",
    "output_csv_file = 'words_frequency.csv'\n",
    "with open(output_csv_file, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['word', 'count'])  # Write header\n",
    "    csvwriter.writerows(sorted_word_counts)  # Write word frequencies\n",
    "\n",
    "print(f\"Word frequencies written to '{output_csv_file}'\")\n",
    "\n",
    "# 16.9s\n",
    "# total words = 12605345, Esperanto words= 12532375, unique words = 371937\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized words count = 261004\n",
      "Normalized word frequencies written to 'words_frequency_normalized.csv'\n"
     ]
    }
   ],
   "source": [
    "# read csv file 'words_frequency.csv' \n",
    "# convert each word to normal form \n",
    "# put normal form into new dictionary, updating frequency, if this form is already in the dictionary\n",
    "# write the result into 'words_frequency_normalized.csv'\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_word(word):\n",
    "    if word.endswith(\"_n\"):\n",
    "        word = word[:-2]\n",
    "    if word.endswith(\"_j\"):\n",
    "        word = word[:-2]\n",
    "    if word.endswith((\"_as\", \"_is\", \"_os\", \"_us\")):\n",
    "        word = word[:-2] + \"i\"\n",
    "    if word.endswith((\"_u\")):\n",
    "        word = word[:-1] + \"i\"\n",
    "    return word\n",
    "\n",
    "# Read the CSV file and normalize words\n",
    "normalized_word_counts = defaultdict(int)\n",
    "\n",
    "with open('words_frequency.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip the header\n",
    "    for row in csvreader:\n",
    "        word, count = row\n",
    "        normalized_word = normalize_word(word)\n",
    "        normalized_word_counts[normalized_word] += int(count)\n",
    "print(f\"normalized words count = {len(normalized_word_counts)}\")\n",
    "\n",
    "# Write the normalized word frequencies to a new CSV file\n",
    "with open('words_frequency_normalized.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['word', 'count'])  # Write header\n",
    "    for word, count in sorted(normalized_word_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        csvwriter.writerow([word, count])\n",
    "\n",
    "print(\"Normalized word frequencies written to 'words_frequency_normalized.csv'\")\n",
    "\n",
    "# normalized words count = 261004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of words with underscores = 199703\n",
      "Words with underscores written to 'words_with_underscore_frequency.csv'\n",
      "count of words without underscores = 61301\n",
      "Words without underscores written to 'words_without_underscore_frequency.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read the file 'words_frequency_normalized.csv' and divide the words into two dictionaries\n",
    "\n",
    "words_with_underscore = defaultdict(int)\n",
    "words_without_underscore = defaultdict(int)\n",
    "\n",
    "# Read the CSV file\n",
    "with open('words_frequency_normalized.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip the header\n",
    "    for row in csvreader:\n",
    "        word, count = row\n",
    "        count = int(count)\n",
    "        if '_' in word:\n",
    "            words_with_underscore[word] += count\n",
    "        else:\n",
    "            words_without_underscore[word] += count\n",
    "\n",
    "# Write the words with underscores to 'words_with_underscore_frequency.csv'\n",
    "print(f'count of words with underscores = {len(words_with_underscore)}')\n",
    "with open('words_with_underscore_frequency.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['word', 'count'])  # Write header\n",
    "    for word, count in sorted(words_with_underscore.items(), key=lambda x: x[1], reverse=True):\n",
    "        csvwriter.writerow([word, count])\n",
    "print(\"Words with underscores written to 'words_with_underscore_frequency.csv'\")\n",
    "\n",
    "# Write the words without underscores to 'words_without_underscore_frequency.csv'\n",
    "print(f'count of words without underscores = {len(words_without_underscore)}')\n",
    "with open('words_without_underscore_frequency.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['word', 'count'])  # Write header\n",
    "    for word, count in sorted(words_without_underscore.items(), key=lambda x: x[1], reverse=True):\n",
    "        csvwriter.writerow([word, count])\n",
    "\n",
    "print(\"Words without underscores written to 'words_without_underscore_frequency.csv'\")\n",
    "\n",
    "# count of words with underscores = 199703\n",
    "# count of words without underscores = 61301\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is manual step: look into words_without_underscore_frequency.csv, select all Esperanto words without endings (numbers, prepositions, correlative words). they are at top. about 160 upper lines.\n",
    "copy them into words_with_underscore_frequency.csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of words = 199577\n",
      "Words with frequencies written to 'words_frequency_1.csv'\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read the file 'words_with_underscore_frequency.csv' into dictionary word-> count\n",
    "words_with_frequencies = defaultdict(int)\n",
    "\n",
    "with open('words_with_underscore_frequency.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip the header\n",
    "    for row in csvreader:\n",
    "        word, count = row\n",
    "        words_with_frequencies[word] += int(count)\n",
    "        \n",
    "# Remove all starting underscores and update the dictionary\n",
    "updated_words_with_frequencies = defaultdict(int)\n",
    "for word, count in words_with_frequencies.items():\n",
    "    word = re.sub(r'__', '_', word)\n",
    "    updated_word = word.lstrip('_').rstrip('_')\n",
    "    updated_words_with_frequencies[updated_word] += int(count)\n",
    "\n",
    "# Sort the words by frequency in descending order, then by word in alphabetical order with locale Esperanto\n",
    "locale.setlocale(locale.LC_ALL, 'eo.UTF-8')\n",
    "words_with_frequencies = sorted(updated_words_with_frequencies.items(), key=lambda x: (-x[1], locale.strxfrm(x[0])))\n",
    "\n",
    "# Write the sorted words into 'words_frequency_1.csv'\n",
    "with open('words_frequency_1.csv', 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['word', 'count'])  # Write header\n",
    "    csvwriter.writerows(words_with_frequencies)  # Write sorted word frequencies\n",
    "\n",
    "print(f'count of words = {len(words_with_frequencies)}')\n",
    "print(\"Words with frequencies written to 'words_frequency_1.csv'\")\n",
    "\n",
    "# 199577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of word parts = 30920\n",
      "Words with frequencies written to 'word_parts_frequency.csv'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import locale\n",
    "import csv\n",
    "\n",
    "# Read the file 'words_frequency_1.csv' into dictionary word-> count\n",
    "words_with_frequencies = defaultdict(int)\n",
    "\n",
    "with open('words_frequency_1.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader)  # Skip the header\n",
    "    for row in csvreader:\n",
    "        word, count = row\n",
    "        words_with_frequencies[word] += int(count)\n",
    "\n",
    "part_frequencies = defaultdict(int)  # Initialize defaultdict with int as default factory\n",
    "for word, frequency in words_with_frequencies.items():\n",
    "    parts = word.split('_')  \n",
    "    num_parts = len(parts)\n",
    "    if num_parts == 1:\n",
    "        part = parts[0]\n",
    "        if part:\n",
    "            part_frequencies[part] += frequency\n",
    "    elif num_parts > 1:\n",
    "        for index, part in enumerate(parts):\n",
    "            if part: \n",
    "                if index == 0: \n",
    "                    formatted_part = part + \"-\"\n",
    "                elif index == num_parts - 1:\n",
    "                    formatted_part = \"-\" + part\n",
    "                else: \n",
    "                    formatted_part = \"-\" + part + \"-\"\n",
    "                part_frequencies[formatted_part] += frequency    \n",
    "\n",
    "# Sort the words by frequency in descending order, then by word in alphabetical order with locale Esperanto\n",
    "locale.setlocale(locale.LC_ALL, 'eo.UTF-8')\n",
    "sorted_part_frequencies = sorted(part_frequencies.items(), key=lambda x: (-x[1], locale.strxfrm(x[0])))\n",
    "\n",
    "output_file = 'word_parts_frequency.csv'\n",
    "# Write the sorted words into 'word_parts_frequency.csv'\n",
    "with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['part', 'count'])  # Write header\n",
    "    csvwriter.writerows(sorted_part_frequencies)  # Write sorted frequencies\n",
    "\n",
    "print(f'count of word parts = {len(sorted_part_frequencies)}')\n",
    "print(f\"Words with frequencies written to '{output_file}'\")\n",
    "\n",
    "# count of word parts = 30920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
